//*****************************************************
//* Grafana Alloy Configuration - gitops-kind
//* Based on k8s-monitoring patterns with standard job labels
//*****************************************************

//*****************************************************
//* DISCOVERY
//*****************************************************

discovery.kubernetes "nodes" {
  role = "node"
}

discovery.kubernetes "services" {
  role = "service"
}

discovery.kubernetes "endpoints" {
  role = "endpoints"
}

discovery.kubernetes "pods" {
  role = "pod"
}

//*****************************************************
//* WRITE ENDPOINTS
//*****************************************************

prometheus.remote_write "default" {
  endpoint {
    url  = "http://mimir-gateway.mimir.svc.cluster.local:80/api/v1/push"
    name = "mimir"

    send_native_histograms = false

    queue_config {
      capacity             = 10000
      min_shards           = 1
      max_shards           = 50
      max_samples_per_send = 2000
      batch_send_deadline  = "5s"
      min_backoff          = "30ms"
      max_backoff          = "5s"
      retry_on_http_429    = true
      sample_age_limit     = "0s"
    }
  }

  wal {
    truncate_frequency = "2h"
    min_keepalive_time = "5m"
    max_keepalive_time = "8h"
  }
}

loki.write "default" {
  endpoint {
    url = "http://loki-gateway.loki.svc.cluster.local:80/loki/api/v1/push"
  }
  external_labels = {
    cluster = "monke-eu-central-1-dev",
  }
}

otelcol.exporter.otlp "tempo" {
  client {
    endpoint = "tempo.tempo.svc.cluster.local:4317"
    tls {
      insecure = true
    }
  }
}

//*****************************************************
//* MODULE: Metrics Relabel (adds cluster label)
//*****************************************************

prometheus.relabel "metrics_service" {
  max_cache_size = 100000
  rule {
    source_labels = ["cluster"]
    regex         = ""
    replacement   = "monke-eu-central-1-dev"
    target_label  = "cluster"
  }
  forward_to = [prometheus.remote_write.default.receiver]
}

//*****************************************************
//* MODULE: OTLP Receivers & Processing Pipeline
//*****************************************************

otelcol.receiver.otlp "receiver" {
  grpc {
    endpoint = "0.0.0.0:4317"
  }
  http {
    endpoint = "0.0.0.0:4318"
  }
  debug_metrics {
    disable_high_cardinality_metrics = true
  }
  output {
    metrics = [otelcol.processor.resourcedetection.default.input]
    logs    = [otelcol.processor.resourcedetection.default.input]
    traces  = [otelcol.processor.resourcedetection.default.input]
  }
}

otelcol.processor.transform "add_metric_datapoint_attributes" {
  error_mode = "ignore"
  metric_statements {
    context = "datapoint"
    statements = [
      "set(attributes[\"deployment.environment\"], resource.attributes[\"deployment.environment\"])",
      "set(attributes[\"service.version\"], resource.attributes[\"service.version\"])",
    ]
  }
  output {
    metrics = [otelcol.processor.k8sattributes.default.input]
  }
}

otelcol.processor.resourcedetection "default" {
  system {
    hostname_sources = ["os"]
  }
  detectors = ["env", "system"]
  output {
    metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]
    logs    = [otelcol.processor.k8sattributes.default.input]
    traces  = [otelcol.processor.k8sattributes.default.input]
  }
}

otelcol.processor.k8sattributes "default" {
  extract {
    metadata = [
      "k8s.namespace.name",
      "k8s.pod.name",
      "k8s.deployment.name",
      "k8s.statefulset.name",
      "k8s.daemonset.name",
      "k8s.cronjob.name",
      "k8s.job.name",
      "k8s.node.name",
      "k8s.pod.uid",
      "k8s.pod.start_time",
    ]
  }
  pod_association {
    source {
      from = "connection"
    }
  }
  output {
    metrics = [otelcol.processor.attributes.default.input]
    logs    = [otelcol.processor.attributes.default.input]
    traces  = [otelcol.processor.attributes.default.input]
  }
}

otelcol.processor.attributes "default" {
  output {
    metrics = [otelcol.processor.transform.default.input]
    logs    = [otelcol.processor.transform.default.input]
    traces  = [
      otelcol.processor.transform.default.input,
      otelcol.connector.host_info.default.input,
    ]
  }
}

otelcol.connector.host_info "default" {
  host_identifiers = ["k8s.node.name"]
  output {
    metrics = [otelcol.processor.batch.host_info_batch.input]
  }
}

otelcol.processor.batch "host_info_batch" {
  output {
    metrics = [otelcol.exporter.prometheus.host_info_metrics.input]
  }
}

otelcol.exporter.prometheus "host_info_metrics" {
  add_metric_suffixes = false
  forward_to          = [prometheus.relabel.metrics_service.receiver]
}

otelcol.processor.transform "default" {
  error_mode = "ignore"
  metric_statements {
    context = "resource"
    statements = [
      "set(attributes[\"k8s.cluster.name\"], \"monke-eu-central-1-dev\") where attributes[\"k8s.cluster.name\"] == nil",
      "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
    ]
  }
  log_statements {
    context = "resource"
    statements = [
      "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
      "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
      "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
      "set(attributes[\"k8s.cluster.name\"], \"monke-eu-central-1-dev\") where attributes[\"k8s.cluster.name\"] == nil",
      "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
    ]
  }
  trace_statements {
    context = "resource"
    statements = [
      "set(attributes[\"k8s.cluster.name\"], \"monke-eu-central-1-dev\") where attributes[\"k8s.cluster.name\"] == nil",
      "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
    ]
  }
  output {
    metrics = [otelcol.processor.filter.default.input]
    logs    = [otelcol.processor.filter.default.input]
    traces  = [otelcol.processor.filter.default.input]
  }
}

otelcol.processor.filter "default" {
  error_mode = "ignore"
  traces {
    span = [
      "attributes[\"http.route\"] == \"/live\"",
      "attributes[\"http.route\"] == \"/healthy\"",
      "attributes[\"http.route\"] == \"/ready\"",
    ]
  }
  output {
    metrics = [otelcol.processor.batch.batch_processor.input]
    logs    = [otelcol.processor.batch.batch_processor.input]
    traces  = [otelcol.processor.batch.batch_processor.input]
  }
}

otelcol.processor.batch "batch_processor" {
  send_batch_size     = 16384
  send_batch_max_size = 0
  timeout             = "2s"
  output {
    metrics = [otelcol.exporter.prometheus.metrics_converter.input]
    logs    = [otelcol.exporter.loki.logs_converter.input]
    traces  = [otelcol.exporter.otlp.tempo.input]
  }
}

otelcol.exporter.prometheus "metrics_converter" {
  add_metric_suffixes = true
  forward_to          = [prometheus.relabel.metrics_service.receiver]
}

otelcol.exporter.loki "logs_converter" {
  forward_to = [loki.process.pod_logs.receiver]
}

//*****************************************************
//* MODULE: Annotation Autodiscovery
//*****************************************************

discovery.relabel "annotation_autodiscovery_pods" {
  targets = discovery.kubernetes.pods.targets
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape"]
    regex         = "true"
    action        = "keep"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_job"]
    action        = "replace"
    target_label  = "job"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_instance"]
    action        = "replace"
    target_label  = "instance"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path"]
    action        = "replace"
    target_label  = "__metrics_path__"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_container_port_name"]
    target_label  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName"]
    regex         = "(.+)"
    target_label  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_container_port_name"]
    action        = "keepequal"
    target_label  = "__tmp_port"
  }
  rule {
    action = "labeldrop"
    regex  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
    regex         = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
    replacement   = "[$2]:$1"
    target_label  = "__address__"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
    regex         = "(\\d+);((([0-9]+?)(\\.|$)){4})"
    replacement   = "$2:$1"
    target_label  = "__address__"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme"]
    action        = "replace"
    target_label  = "__scheme__"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeInterval"]
    action        = "replace"
    target_label  = "__scrape_interval__"
  }
}

discovery.relabel "annotation_autodiscovery_services" {
  targets = discovery.kubernetes.services.targets
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_scrape"]
    regex         = "true"
    action        = "keep"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_job"]
    action        = "replace"
    target_label  = "job"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_instance"]
    action        = "replace"
    target_label  = "instance"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path"]
    action        = "replace"
    target_label  = "__metrics_path__"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_port_name"]
    target_label  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName"]
    regex         = "(.+)"
    target_label  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_port_name"]
    action        = "keepequal"
    target_label  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_port_number"]
    target_label  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber"]
    regex         = "(.+)"
    target_label  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_port_number"]
    action        = "keepequal"
    target_label  = "__tmp_port"
  }
  rule {
    action = "labeldrop"
    regex  = "__tmp_port"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme"]
    action        = "replace"
    target_label  = "__scheme__"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeInterval"]
    action        = "replace"
    target_label  = "__scrape_interval__"
  }
}

discovery.relabel "annotation_autodiscovery_http" {
  targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
  rule {
    source_labels = ["__scheme__"]
    regex         = "https"
    action        = "drop"
  }
}

discovery.relabel "annotation_autodiscovery_https" {
  targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
  rule {
    source_labels = ["__scheme__"]
    regex         = "https"
    action        = "keep"
  }
}

prometheus.scrape "annotation_autodiscovery_http" {
  targets           = discovery.relabel.annotation_autodiscovery_http.output
  scrape_interval   = "60s"
  honor_labels      = true
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]
}

prometheus.scrape "annotation_autodiscovery_https" {
  targets           = discovery.relabel.annotation_autodiscovery_https.output
  scrape_interval   = "60s"
  honor_labels      = true
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]
}

prometheus.relabel "annotation_autodiscovery" {
  max_cache_size = 100000
  forward_to     = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: Alloy Self-Monitoring
//*****************************************************

discovery.relabel "alloy" {
  targets = discovery.kubernetes.pods.targets
  rule {
    source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
    regex         = "alloy.*"
    action        = "keep"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_container_port_name"]
    regex         = "http-metrics"
    action        = "keep"
  }
  rule {
    source_labels = ["__meta_kubernetes_namespace"]
    target_label  = "namespace"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_name"]
    target_label  = "pod"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_container_name"]
    target_label  = "container"
  }
}

prometheus.scrape "alloy" {
  job_name        = "alloy"
  targets         = discovery.relabel.alloy.output
  scrape_interval = "60s"
  forward_to      = [prometheus.relabel.alloy.receiver]
  clustering {
    enabled = true
  }
}

prometheus.relabel "alloy" {
  max_cache_size = 100000
  rule {
    source_labels = ["__name__"]
    regex         = "up|alloy_build_info|cluster_node_peers"
    action        = "keep"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: Kubelet
//*****************************************************

discovery.relabel "kubelet" {
  targets = discovery.kubernetes.nodes.targets
}

prometheus.scrape "kubelet" {
  job_name          = "kubelet"
  targets           = discovery.relabel.kubelet.output
  scheme            = "https"
  scrape_interval   = "60s"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.kubelet.receiver]
}

prometheus.relabel "kubelet" {
  max_cache_size = 100000
  rule {
    source_labels = ["__name__"]
    regex         = "up|go_goroutines|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_free|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kubernetes_build_info|namespace_workload_pod|process_cpu_seconds_total|process_resident_memory_bytes|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
    action        = "keep"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: Kubelet Resource
//*****************************************************

discovery.relabel "kubelet_resource" {
  targets = discovery.kubernetes.nodes.targets
  rule {
    replacement  = "/metrics/resource"
    target_label = "__metrics_path__"
  }
}

prometheus.scrape "kubelet_resource" {
  job_name          = "kubelet"
  targets           = discovery.relabel.kubelet_resource.output
  scheme            = "https"
  scrape_interval   = "60s"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.kubelet_resource.receiver]
}

prometheus.relabel "kubelet_resource" {
  max_cache_size = 100000
  rule {
    source_labels = ["__name__"]
    regex         = "up|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
    action        = "keep"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: cAdvisor
//*****************************************************

discovery.relabel "cadvisor" {
  targets = discovery.kubernetes.nodes.targets
  rule {
    replacement  = "/metrics/cadvisor"
    target_label = "__metrics_path__"
  }
}

prometheus.scrape "cadvisor" {
  job_name          = "cadvisor"
  targets           = discovery.relabel.cadvisor.output
  scheme            = "https"
  scrape_interval   = "60s"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.cadvisor.receiver]
}

prometheus.relabel "cadvisor" {
  max_cache_size = 100000
  rule {
    source_labels = ["__name__"]
    regex         = "up|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_usage_bytes|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
    action        = "keep"
  }
  // Drop empty container labels
  rule {
    source_labels = ["__name__", "container"]
    separator     = "@"
    regex         = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
    action        = "drop"
  }
  // Drop empty image labels
  rule {
    source_labels = ["__name__", "image"]
    separator     = "@"
    regex         = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
    action        = "drop"
  }
  // Normalizing unimportant labels
  rule {
    source_labels = ["__name__", "boot_id"]
    separator     = "@"
    regex         = "machine_memory_bytes@.*"
    target_label  = "boot_id"
    replacement   = "NA"
  }
  rule {
    source_labels = ["__name__", "system_uuid"]
    separator     = "@"
    regex         = "machine_memory_bytes@.*"
    target_label  = "system_uuid"
    replacement   = "NA"
  }
  // Filter out non-physical devices
  rule {
    source_labels = ["__name__", "device"]
    separator     = "@"
    regex         = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
    target_label  = "__keepme"
    replacement   = "1"
  }
  rule {
    source_labels = ["__name__", "__keepme"]
    separator     = "@"
    regex         = "container_fs_.*@"
    action        = "drop"
  }
  rule {
    source_labels = ["__name__"]
    regex         = "container_fs_.*"
    target_label  = "__keepme"
    replacement   = ""
  }
  // Filter out non-physical interfaces
  rule {
    source_labels = ["__name__", "interface"]
    separator     = "@"
    regex         = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
    target_label  = "__keepme"
    replacement   = "1"
  }
  rule {
    source_labels = ["__name__", "__keepme"]
    separator     = "@"
    regex         = "container_network_.*@"
    action        = "drop"
  }
  rule {
    source_labels = ["__name__"]
    regex         = "container_network_.*"
    target_label  = "__keepme"
    replacement   = ""
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: kube-state-metrics
//*****************************************************

discovery.relabel "kube_state_metrics" {
  targets = discovery.kubernetes.services.targets
  rule {
    source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
    regex         = "kube-state-metrics"
    action        = "keep"
  }
  rule {
    source_labels = ["__meta_kubernetes_service_port_name"]
    regex         = "http"
    action        = "keep"
  }
}

prometheus.scrape "kube_state_metrics" {
  job_name        = "kube-state-metrics"
  targets         = discovery.relabel.kube_state_metrics.output
  scrape_interval = "60s"
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.kube_state_metrics.receiver]
}

prometheus.relabel "kube_state_metrics" {
  max_cache_size = 100000
  rule {
    source_labels = ["__name__"]
    regex         = "up|kube_configmap_info|kube_configmap_metadata_resource_version|kube_cronjob.*|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_condition|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolume_status_phase|kube_persistentvolumeclaim_access_mode|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_labels|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolumeclaim_status_phase|kube_pod_completion_time|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_last_terminated_timestamp|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_restart_policy|kube_pod_spec_volumes_persistentvolumeclaims_info|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_replicaset.*|kube_resourcequota|kube_secret_metadata_resource_version|kube_statefulset.*"
    action        = "keep"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: Node Exporter (built-in)
//*****************************************************

prometheus.exporter.unix "node" {
  procfs_path = "/host/proc"
  sysfs_path  = "/host/sys"
  rootfs_path = "/host/root"

  include_exporter_metrics = true
}

discovery.relabel "node_exporter" {
  targets = prometheus.exporter.unix.node.targets
  rule {
    target_label = "job"
    replacement  = "node-exporter"
  }
  rule {
    target_label = "instance"
    replacement  = env("HOSTNAME")
  }
}

prometheus.scrape "node_exporter" {
  job_name        = "node-exporter"
  targets         = discovery.relabel.node_exporter.output
  scrape_interval = "60s"
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.node_exporter.receiver]
}

prometheus.relabel "node_exporter" {
  max_cache_size = 100000
  rule {
    source_labels = ["__name__"]
    regex         = "up|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
    action        = "keep"
  }
  // Drop metrics for certain file systems
  rule {
    source_labels = ["__name__", "fstype"]
    separator     = "@"
    regex         = "node_filesystem.*@(ramfs|tmpfs)"
    action        = "drop"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: Control Plane (kube-apiserver only in Kind)
//*****************************************************

discovery.kubernetes "kube_apiserver" {
  role = "endpoints"

  namespaces {
    names = ["default"]
  }

  selectors {
    role  = "endpoints"
    field = "metadata.name=kubernetes"
  }
}

discovery.relabel "kube_apiserver" {
  targets = discovery.kubernetes.kube_apiserver.targets
  rule {
    target_label = "job"
    replacement  = "kube-apiserver"
  }
}

prometheus.scrape "kube_apiserver" {
  job_name          = "kube-apiserver"
  targets           = discovery.relabel.kube_apiserver.output
  scheme            = "https"
  scrape_interval   = "60s"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: Prometheus Operator Objects
//*****************************************************

prometheus.operator.podmonitors "pod_monitors" {
  clustering {
    enabled = true
  }
  scrape {
    default_scrape_interval = "60s"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

prometheus.operator.probes "probes" {
  clustering {
    enabled = true
  }
  scrape {
    default_scrape_interval = "60s"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

prometheus.operator.servicemonitors "service_monitors" {
  clustering {
    enabled = true
  }
  scrape {
    default_scrape_interval = "60s"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

//*****************************************************
//* MODULE: Pod Logs
//*****************************************************

discovery.relabel "pod_logs" {
  targets = discovery.kubernetes.pods.targets

  rule {
    source_labels = ["__meta_kubernetes_namespace"]
    action        = "replace"
    target_label  = "namespace"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_name"]
    action        = "replace"
    target_label  = "pod"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_container_name"]
    action        = "replace"
    target_label  = "container"
  }
  rule {
    source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
    separator     = "/"
    action        = "replace"
    replacement   = "$1"
    target_label  = "job"
  }
  rule {
    action       = "replace"
    source_labels = ["__meta_kubernetes_pod_container_id"]
    regex        = "^(\\S+):\\/\\/.+$"
    replacement  = "$1"
    target_label = "tmp_container_runtime"
  }
  rule {
    action = "labelmap"
    regex  = "__meta_kubernetes_pod_label_(.+)"
  }
  rule {
    action = "labelmap"
    regex  = "__meta_kubernetes_pod_annotation_(.+)"
  }
  rule {
    source_labels = [
      "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
      "__meta_kubernetes_pod_label_app_kubernetes_io_name",
      "__meta_kubernetes_pod_container_name",
    ]
    separator    = ";"
    regex        = "^(?:;*)?([^;]+).*$"
    replacement  = "$1"
    target_label = "service_name"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
    separator     = "/"
    action        = "replace"
    replacement   = "/var/log/pods/*$1/*.log"
    target_label  = "__path__"
  }
}

loki.source.kubernetes "pod_logs" {
  targets    = discovery.relabel.pod_logs.output
  forward_to = [loki.process.pod_logs.receiver]
}

loki.process "pod_logs" {
  stage.match {
    selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
    stage.cri {}
    stage.labels {
      values = {
        flags  = "",
        stream = "",
      }
    }
  }

  stage.match {
    selector = "{tmp_container_runtime=\"docker\"}"
    stage.docker {}
    stage.labels {
      values = {
        stream = "",
      }
    }
  }

  stage.label_drop {
    values = [
      "filename",
      "tmp_container_runtime",
    ]
  }

  forward_to = [loki.write.default.receiver]
}

//*****************************************************
//* MODULE: Node Logs (systemd journal)
//*****************************************************

loki.source.journal "node_logs" {
  path = "/var/log/journal"

  labels = {
    job = "node/journal",
  }

  forward_to = [loki.write.default.receiver]
}

//*****************************************************
//* MODULE: Cluster Events
//*****************************************************

loki.source.kubernetes_events "cluster_events" {
  job_name   = "integrations/kubernetes/eventhandler"
  log_format = "logfmt"
  forward_to = [loki.process.cluster_events.receiver]
}

loki.process "cluster_events" {
  stage.static_labels {
    values = {
      source = "kubernetes-events",
    }
  }

  stage.logfmt {
    mapping = {
      component = "sourcecomponent",
      kind      = "",
      level     = "type",
      name      = "",
      node      = "sourcehost",
      reason    = "",
    }
  }

  stage.labels {
    values = {
      component = "",
      kind      = "",
      level     = "",
      name      = "",
      node      = "",
      reason    = "",
    }
  }

  stage.match {
    selector = "{kind=\"Node\"}"
    stage.labels {
      values = {
        node = "name",
      }
    }
  }

  stage.match {
    selector = "{level=\"Normal\"}"
    stage.static_labels {
      values = {
        level = "Info",
      }
    }
  }

  stage.structured_metadata {
    values = {
      name = "name",
    }
  }

  stage.label_keep {
    values = ["job", "level", "namespace", "node", "source", "reason"]
  }

  forward_to = [loki.write.default.receiver]
}

//*****************************************************
//* MODULE: Mimir Rules
//*****************************************************

mimir.rules.kubernetes "default" {
  address = "http://mimir-ruler.mimir.svc.cluster.local:8080"
}

//*****************************************************
//* LOGGING
//*****************************************************

logging {
  level  = "info"
  format = "logfmt"
}
