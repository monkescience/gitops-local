discovery.kubernetes "nodes" {
  role = "node"
}

discovery.kubernetes "services" {
  role = "service"
}

discovery.kubernetes "endpointslices" {
  role = "endpointslice"
}

discovery.kubernetes "pods" {
  role = "pod"
}

prometheus.remote_write "mimir" {
  endpoint {
    url                    = "http://host.docker.internal:8080/mimir/api/v1/push"
    name                   = "mimir"
    send_native_histograms = true
  }
}

loki.write "loki" {
  endpoint {
    url = "http://host.docker.internal:8080/loki/api/v1/push"
  }
  external_labels = {
    cluster = sys.env("CLUSTER_NAME"),
  }
}

otelcol.exporter.otlp "tempo" {
  client {
    endpoint = "host.docker.internal:8080"
    tls {
      insecure = true
    }
  }
}

prometheus.relabel "metrics_service" {
  rule {
    source_labels = ["cluster"]
    regex         = ""
    replacement   = sys.env("CLUSTER_NAME")
    target_label  = "cluster"
  }
  forward_to = [prometheus.remote_write.mimir.receiver]
}

mimir.rules.kubernetes "rules" {
  address                = "http://host.docker.internal:8080/mimir-ruler"
  mimir_namespace_prefix = sys.env("CLUSTER_NAME")
}

logging {
  level  = "info"
  format = "logfmt"
}

prometheus.scrape "kubelet" {
  job_name          = "kubelet"
  targets           = discovery.kubernetes.nodes.targets
  scheme            = "https"
  scrape_interval   = "60s"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.kubelet.receiver]
}

prometheus.relabel "kubelet" {
  rule {
    source_labels = ["__name__"]
    regex         = "up|go_goroutines|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_free|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kubernetes_build_info|namespace_workload_pod|process_cpu_seconds_total|process_resident_memory_bytes|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
    action        = "keep"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

discovery.relabel "kubelet_resource" {
  targets = discovery.kubernetes.nodes.targets
  rule {
    replacement  = "/metrics/resource"
    target_label = "__metrics_path__"
  }
}

prometheus.scrape "kubelet_resource" {
  job_name          = "kubelet"
  targets           = discovery.relabel.kubelet_resource.output
  scheme            = "https"
  scrape_interval   = "60s"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.kubelet_resource.receiver]
}

prometheus.relabel "kubelet_resource" {
  rule {
    source_labels = ["__name__"]
    regex         = "up|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
    action        = "keep"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

discovery.relabel "cadvisor" {
  targets = discovery.kubernetes.nodes.targets
  rule {
    replacement  = "/metrics/cadvisor"
    target_label = "__metrics_path__"
  }
}

prometheus.scrape "cadvisor" {
  job_name          = "cadvisor"
  targets           = discovery.relabel.cadvisor.output
  scheme            = "https"
  scrape_interval   = "60s"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.cadvisor.receiver]
}

prometheus.relabel "cadvisor" {
  rule {
    source_labels = ["__name__"]
    regex         = "up|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_usage_bytes|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
    action        = "keep"
  }
  rule {
    source_labels = ["__name__", "container"]
    separator     = "@"
    regex         = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
    action        = "drop"
  }
  rule {
    source_labels = ["__name__", "image"]
    separator     = "@"
    regex         = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
    action        = "drop"
  }
  rule {
    source_labels = ["__name__", "boot_id"]
    separator     = "@"
    regex         = "machine_memory_bytes@.*"
    target_label  = "boot_id"
    replacement   = "NA"
  }
  rule {
    source_labels = ["__name__", "system_uuid"]
    separator     = "@"
    regex         = "machine_memory_bytes@.*"
    target_label  = "system_uuid"
    replacement   = "NA"
  }
  rule {
    source_labels = ["__name__", "device"]
    separator     = "@"
    regex         = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
    target_label  = "__keepme"
    replacement   = "1"
  }
  rule {
    source_labels = ["__name__", "__keepme"]
    separator     = "@"
    regex         = "container_fs_.*@"
    action        = "drop"
  }
  rule {
    source_labels = ["__name__"]
    regex         = "container_fs_.*"
    target_label  = "__keepme"
    replacement   = ""
  }
  rule {
    source_labels = ["__name__", "interface"]
    separator     = "@"
    regex         = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
    target_label  = "__keepme"
    replacement   = "1"
  }
  rule {
    source_labels = ["__name__", "__keepme"]
    separator     = "@"
    regex         = "container_network_.*@"
    action        = "drop"
  }
  rule {
    source_labels = ["__name__"]
    regex         = "container_network_.*"
    target_label  = "__keepme"
    replacement   = ""
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

prometheus.exporter.unix "node" {
  procfs_path = "/host/proc"
  sysfs_path  = "/host/sys"
  rootfs_path = "/host/root"

  include_exporter_metrics = true
}

discovery.relabel "node_exporter" {
  targets = prometheus.exporter.unix.node.targets
  rule {
    target_label = "job"
    replacement  = "node-exporter"
  }
  rule {
    target_label = "instance"
    replacement  = sys.env("HOSTNAME")
  }
}

prometheus.scrape "node_exporter" {
  job_name        = "node-exporter"
  targets         = discovery.relabel.node_exporter.output
  scrape_interval = "60s"
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.node_exporter.receiver]
}

prometheus.relabel "node_exporter" {
  rule {
    source_labels = ["__name__", "fstype"]
    separator     = "@"
    regex         = "node_filesystem.*@(ramfs|tmpfs)"
    action        = "drop"
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

discovery.kubernetes "kube_apiserver" {
  role = "endpointslice"

  namespaces {
    names = ["default"]
  }

  selectors {
    role  = "endpointslice"
    label = "kubernetes.io/service-name=kubernetes"
  }
}

discovery.relabel "kube_apiserver" {
  targets = discovery.kubernetes.kube_apiserver.targets
  rule {
    target_label = "job"
    replacement  = "kube-apiserver"
  }
}

prometheus.scrape "kube_apiserver" {
  job_name          = "kube-apiserver"
  targets           = discovery.relabel.kube_apiserver.output
  scheme            = "https"
  scrape_interval   = "60s"
  bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  tls_config {
    insecure_skip_verify = true
  }
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

prometheus.operator.podmonitors "pod_monitors" {
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

prometheus.operator.probes "probes" {
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

prometheus.operator.servicemonitors "service_monitors" {
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.metrics_service.receiver]
}

discovery.relabel "pod_logs" {
  targets = discovery.kubernetes.pods.targets

  rule {
    source_labels = ["__meta_kubernetes_namespace"]
    action        = "replace"
    target_label  = "namespace"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_name"]
    action        = "replace"
    target_label  = "pod"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_container_name"]
    action        = "replace"
    target_label  = "container"
  }
  rule {
    source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
    separator     = "/"
    action        = "replace"
    replacement   = "$1"
    target_label  = "job"
  }
  rule {
    action        = "replace"
    source_labels = ["__meta_kubernetes_pod_container_id"]
    regex         = "^(\\S+):\\/\\/.+$"
    replacement   = "$1"
    target_label  = "tmp_container_runtime"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
    target_label  = "app"
  }
  rule {
    source_labels = [
      "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
      "__meta_kubernetes_pod_label_app_kubernetes_io_name",
      "__meta_kubernetes_pod_container_name",
    ]
    separator    = ";"
    regex        = "^(?:;*)?([^;]+).*$"
    replacement  = "$1"
    target_label = "service_name"
  }
  rule {
    source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
    separator     = "/"
    action        = "replace"
    replacement   = "/var/log/pods/*$1/*.log"
    target_label  = "__path__"
  }
}

loki.source.kubernetes "pod_logs" {
  targets    = discovery.relabel.pod_logs.output
  forward_to = [loki.process.pod_logs.receiver]
}

loki.process "pod_logs" {
  stage.match {
    selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
    stage.cri {}
    stage.labels {
      values = {
        flags  = "",
        stream = "",
      }
    }
  }

  stage.match {
    selector = "{tmp_container_runtime=\"docker\"}"
    stage.docker {}
    stage.labels {
      values = {
        stream = "",
      }
    }
  }

  stage.label_drop {
    values = [
      "filename",
      "tmp_container_runtime",
    ]
  }

  forward_to = [loki.write.loki.receiver]
}

loki.source.journal "node_logs" {
  path = "/var/log/journal"

  labels = {
    job = "node/journal",
  }

  forward_to = [loki.write.loki.receiver]
}

loki.source.kubernetes_events "cluster_events" {
  job_name   = "integrations/kubernetes/eventhandler"
  forward_to = [loki.process.cluster_events.receiver]
}

loki.process "cluster_events" {
  stage.static_labels {
    values = {
      source = "kubernetes-events",
    }
  }

  stage.logfmt {
    mapping = {
      component = "sourcecomponent",
      kind      = "",
      level     = "type",
      name      = "",
      node      = "sourcehost",
      reason    = "",
    }
  }

  stage.labels {
    values = {
      component = "",
      kind      = "",
      level     = "",
      name      = "",
      node      = "",
      reason    = "",
    }
  }

  stage.match {
    selector = "{kind=\"Node\"}"
    stage.labels {
      values = {
        node = "name",
      }
    }
  }

  stage.match {
    selector = "{level=\"Normal\"}"
    stage.static_labels {
      values = {
        level = "Info",
      }
    }
  }

  stage.structured_metadata {
    values = {
      name = "name",
    }
  }

  stage.label_keep {
    values = ["job", "level", "namespace", "node", "source", "reason"]
  }

  forward_to = [loki.write.loki.receiver]
}

otelcol.receiver.otlp "receiver" {
  grpc {
    endpoint = "0.0.0.0:4317"
  }
  http {
    endpoint = "0.0.0.0:4318"
  }
  debug_metrics {
    disable_high_cardinality_metrics = true
  }
  output {
    metrics = [otelcol.processor.resourcedetection.resource.input]
    logs    = [otelcol.processor.resourcedetection.resource.input]
    traces  = [otelcol.processor.resourcedetection.resource.input]
  }
}

otelcol.processor.resourcedetection "resource" {
  detectors = ["env", "system"]
  output {
    metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]
    logs    = [otelcol.processor.k8sattributes.k8s.input]
    traces  = [otelcol.processor.k8sattributes.k8s.input]
  }
}

otelcol.processor.transform "add_metric_datapoint_attributes" {
  error_mode = "ignore"
  metric_statements {
    context = "datapoint"
    statements = [
      "set(attributes[\"deployment.environment\"], resource.attributes[\"deployment.environment\"])",
      "set(attributes[\"service.version\"], resource.attributes[\"service.version\"])",
    ]
  }
  output {
    metrics = [otelcol.processor.k8sattributes.k8s.input]
  }
}

otelcol.processor.k8sattributes "k8s" {
  extract {
    metadata = [
      "k8s.namespace.name",
      "k8s.pod.name",
      "k8s.deployment.name",
      "k8s.statefulset.name",
      "k8s.daemonset.name",
      "k8s.cronjob.name",
      "k8s.job.name",
      "k8s.node.name",
      "k8s.pod.uid",
      "k8s.pod.start_time",
    ]
  }
  pod_association {
    source {
      from = "connection"
    }
  }
  output {
    metrics = [otelcol.processor.transform.enrich.input]
    logs    = [otelcol.processor.transform.enrich.input]
    traces  = [
      otelcol.processor.transform.enrich.input,
      otelcol.connector.host_info.node.input,
    ]
  }
}

otelcol.connector.host_info "node" {
  host_identifiers = ["k8s.node.name"]
  output {
    metrics = [otelcol.processor.batch.host_info_batch.input]
  }
}

otelcol.processor.batch "host_info_batch" {
  output {
    metrics = [otelcol.exporter.prometheus.host_info_metrics.input]
  }
}

otelcol.exporter.prometheus "host_info_metrics" {
  add_metric_suffixes = false
  forward_to          = [prometheus.relabel.metrics_service.receiver]
}

otelcol.processor.transform "enrich" {
  error_mode = "ignore"
  metric_statements {
    context = "resource"
    statements = [
      "set(attributes[\"k8s.cluster.name\"], \"" + sys.env("CLUSTER_NAME") + "\") where attributes[\"k8s.cluster.name\"] == nil",
      "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
    ]
  }
  log_statements {
    context = "resource"
    statements = [
      "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
      "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
      "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
      "set(attributes[\"k8s.cluster.name\"], \"" + sys.env("CLUSTER_NAME") + "\") where attributes[\"k8s.cluster.name\"] == nil",
      "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
    ]
  }
  trace_statements {
    context = "resource"
    statements = [
      "set(attributes[\"k8s.cluster.name\"], \"" + sys.env("CLUSTER_NAME") + "\") where attributes[\"k8s.cluster.name\"] == nil",
      "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
    ]
  }
  output {
    metrics = [otelcol.processor.filter.drop_health.input]
    logs    = [otelcol.processor.filter.drop_health.input]
    traces  = [otelcol.processor.filter.drop_health.input]
  }
}

otelcol.processor.filter "drop_health" {
  error_mode = "ignore"
  traces {
    span = [
      "attributes[\"http.route\"] == \"/live\"",
      "attributes[\"http.route\"] == \"/healthy\"",
      "attributes[\"http.route\"] == \"/ready\"",
    ]
  }
  output {
    metrics = [otelcol.processor.batch.batch_processor.input]
    logs    = [otelcol.processor.batch.batch_processor.input]
    traces  = [otelcol.processor.batch.batch_processor.input]
  }
}

otelcol.processor.batch "batch_processor" {
  send_batch_size     = 16384
  send_batch_max_size = 0
  timeout             = "2s"
  output {
    metrics = [otelcol.exporter.prometheus.metrics_converter.input]
    logs    = [otelcol.exporter.loki.logs_converter.input]
    traces  = [otelcol.exporter.otlp.tempo.input]
  }
}

otelcol.exporter.prometheus "metrics_converter" {
  add_metric_suffixes = true
  forward_to          = [prometheus.relabel.metrics_service.receiver]
}

otelcol.exporter.loki "logs_converter" {
  forward_to = [loki.process.pod_logs.receiver]
}
